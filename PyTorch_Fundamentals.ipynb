{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXjNi5o32UjWkTxk06RBg8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaddobea/PyTorch-Fundamentals/blob/main/PyTorch_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **PyTorch Fundamentals: Dive into Hands-on Deep Learning**\n",
        "==============================================================\n",
        "\n",
        "**Get Started with PyTorch: Tensors and Operations**\n",
        "-----------------------------------------------------------\n",
        "\n",
        "> This notebook is designed to introduce you to the fundamentals of PyTorch, a popular deep learning framework. Through clear examples and detailed explanations, you'll learn the basics of:\n",
        "\n",
        "* Tensor initialization\n",
        "* Tensor operations\n",
        "* Indexing and slicing\n",
        "* Reshaping tensors\n",
        "\n",
        "Follow along to gain hands-on experience with PyTorch and set yourself up for success in your deep learning journey.\n"
      ],
      "metadata": {
        "id": "uo3PeIFu8NL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "### Tensors Fundamentals\n",
        "\n",
        "* [What are Tensors?](#What-are-Tensors?)\n",
        "* [Tensor Initialization](#Tensor-Initialization)\n",
        "* [Common Tensor Initialization Methods](#Common-Tensor-Initialization-Methods)\n",
        "\n",
        "### Tensor Manipulation\n",
        "\n",
        "* [Tensor Type Conversion](#Tensor-Type-Conversion)\n",
        "* [Converting Between NumPy Arrays and Tensors](#Converting-Between-NumPy-Arrays-and-Tensors)\n",
        "\n",
        "### Tensor Operations\n",
        "\n",
        "* [Tensor Mathematics and Comparison Operations](#Tensor-Mathematics-and-Comparison-Operations)\n",
        "* [Matrix Multiplication and Batch Operations](#Matrix-Multiplication-and-Batch-Operations)\n",
        "* [Broadcasting and Other Useful Operations](#Broadcasting-and-Other-Useful-Operations)\n",
        "\n",
        "### Tensor Transformation\n",
        "\n",
        "* [Tensor Indexing](#Tensor-Indexing)\n",
        "* [Tensor Reshaping](#Tensor-Reshaping)"
      ],
      "metadata": {
        "id": "URPPgKZAAEqq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wmSzTwqcL3TD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1291dc21-86e3-4b68-ac9f-256135fc9083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2Mbp7MhUm6X",
        "outputId": "8346ef87-5769-4cb6-f0dc-ea2f2b9c09c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Print versions\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"numpy version:\", np.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obCOQPouVFOl",
        "outputId": "b6411680-7835-4944-d8d5-ae4661dab082"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.6.0+cu124\n",
            "numpy version: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialising a tensor\n",
        "\n",
        "The following code demonstrates how to create a 2×3 PyTorch tensor with a float32 data type, place it on either CPU or GPU (depending on availability), and enable gradient tracking for automatic differentiation."
      ],
      "metadata": {
        "id": "QfkSAeBjWoH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for CUDA availability and set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize a 2x3 tensor with requires_grad enabled\n",
        "my_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32, device=device, requires_grad=True)\n",
        "\n",
        "print(my_tensor)\n",
        "print(\"Data type:\", my_tensor.dtype)\n",
        "print(\"Device:\", my_tensor.device)\n",
        "print(\"Shape:\", my_tensor.shape)\n",
        "print(\"Requires Gradient:\", my_tensor.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aeu_v4QXx90",
        "outputId": "8b95558e-dc7b-459d-fa08-06f876c4c8b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], device='cuda:0', requires_grad=True)\n",
            "Data type: torch.float32\n",
            "Device: cuda:0\n",
            "Shape: torch.Size([2, 3])\n",
            "Requires Gradient: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Common Tensor Methods\n",
        "\n",
        "\n",
        "1. **Empty Tensor:** How to create an unintialised 3x3 tensor filled with zeros.\n",
        "2. **Zero Tensor:** Create a Tensor filled with only zeros.\n",
        "3. **Random Tensor:** How to create a tensor filled with random values varrying between 0 and 1.\n",
        "4. **Ones Tensor:** How to create a Tensor filled with only ones.\n",
        "5. **Identity Matrix:** This type of tensor generates a 4x4 identity matrix  with diagonal consisting of ones.\n",
        "6. **Arange Tensor:** Creates a 1-dimensional tensor with values ranging from 0 to 4 with an increment of 1.\n",
        "7. **Linespace Tensor:** This type of tensor generate a 5x5 evenly spaced tensor with values ranging between 0.1 and 1.\n",
        "8. **Normal Distributed Tensor:**  This tensor fills a tensor with values from a noraml (Gaussian) distribution with mean 0 and a standard deviation of 1.\n",
        "9. **Uniform Distributed Tensor:** It's a tensor filled with values from a uniform distribution between O and 1.\n",
        "10. **Diagonal Tensor:** This type of tensor creates a 4x4 diagonal tensor with ones along the diagonal and zeros.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZwdJ4kuHX8Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a 6x6 empty tensor\n",
        "x = torch.empty(6,6)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFycXCy4X04b",
        "outputId": "4394695a-4a0b-401e-c369-fa2c5ed37c3e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor filled with zeros\n",
        "x = torch.zeros(6, 6)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5X1FDtMbvnf",
        "outputId": "cab81833-f528-48e3-eaae-090a95f71557"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates a tensor with random values\n",
        "x=torch.rand(6,6)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EIoE_CSb7nw",
        "outputId": "055f8b08-dac4-4369-9346-70bdf3161aea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6298, 0.5027, 0.9704, 0.7509, 0.9250, 0.5062],\n",
              "        [0.8393, 0.0021, 0.0624, 0.4847, 0.8281, 0.0524],\n",
              "        [0.2878, 0.2016, 0.8716, 0.2949, 0.9506, 0.9160],\n",
              "        [0.6528, 0.5802, 0.1824, 0.2500, 0.6889, 0.9897],\n",
              "        [0.4096, 0.1905, 0.3617, 0.9430, 0.6647, 0.8725],\n",
              "        [0.1612, 0.9256, 0.5209, 0.6850, 0.6689, 0.7762]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates a tensor filled with only ones\n",
        "x=torch.ones(6,6)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7nnte-IcLqY",
        "outputId": "308c4a9f-abfa-4c87-c578-5da7895ff2a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an identity Matrix\n",
        "x=torch.eye(6,6)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOcTt9DRcdvF",
        "outputId": "068a7149-a974-45a2-a80b-b7f56591267b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a tensor using arange\n",
        "x=torch.arange(8,dtype=torch.float32)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn9xgSFEcwS6",
        "outputId": "0e8b2b5e-627f-4f4e-dd2c-f705cc0d5023"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3., 4., 5., 6., 7.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a linspace tensor\n",
        "x = torch.linspace(0.5, 5, steps=10)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlKcS-VfdpDD",
        "outputId": "cf2c813c-6164-48ca-dd26-945ead9340e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000, 4.5000,\n",
              "        5.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A normal distribution tensor\n",
        "x=torch.normal(mean=4,std=2,size=(6,6))\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhZoACnTeGJL",
        "outputId": "b4e64eae-997b-4474-c0a5-a894b37e73a4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.4764,  0.9199,  5.9423,  2.6484, -0.0114,  5.2768],\n",
              "        [ 3.0782,  4.2619,  2.5181,  1.9943,  2.7766,  0.1989],\n",
              "        [ 3.0370,  5.9030,  6.1878,  3.3009, -0.6036,  2.6345],\n",
              "        [ 3.4277,  5.4019,  0.4103,  2.8914,  6.7001,  0.5340],\n",
              "        [ 8.3322,  1.5504,  6.7575,  3.2842,  2.2773,  0.5482],\n",
              "        [ 2.4567,  4.2741,  3.8844,  6.8725,  3.4687,  5.4158]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# still on a normal distribution tensor\n",
        "x=torch.empty(1,5).normal_(mean=4,std=2)\n",
        "x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npGh1hKAeVAk",
        "outputId": "e625b941-b548-4e97-fcb3-4901a4b92d88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.0910, -1.3339,  5.0517,  1.8393, -0.1877]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A uniform distribution tensor\n",
        "x = torch.empty(1, 5).uniform_(0, 1)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-4Vi9GgexWU",
        "outputId": "88cec6d6-5221-4772-b6c0-4ec8900c81a2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0914, 0.5603, 0.0864, 0.0635, 0.7046]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagonal tensor\n",
        "x=torch.diag(torch.ones(6))\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPEaznEil6YU",
        "outputId": "9be3365d-a601-4024-c28b-0a4d97c32dfc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Type  Conversion\n",
        "\n",
        "Here, we create a tensor with values [0,1,2,3] and also demonstrates type conversion to boolean, int16,int64,float16,float32 and float64."
      ],
      "metadata": {
        "id": "U5kFXOaumU7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.arange (6)\n",
        "print(tensor)\n",
        "print(tensor.dtype)\n",
        "print(tensor.bool())\n",
        "print(tensor.short())\n",
        "print(tensor.long())\n",
        "print(tensor.half())\n",
        "print(tensor.float())\n",
        "print(tensor.double())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mvG2ahqm0gH",
        "outputId": "aabce400-c664-4286-b9d0-39313fec33a8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5])\n",
            "torch.int64\n",
            "tensor([False,  True,  True,  True,  True,  True])\n",
            "tensor([0, 1, 2, 3, 4, 5], dtype=torch.int16)\n",
            "tensor([0, 1, 2, 3, 4, 5])\n",
            "tensor([0., 1., 2., 3., 4., 5.], dtype=torch.float16)\n",
            "tensor([0., 1., 2., 3., 4., 5.])\n",
            "tensor([0., 1., 2., 3., 4., 5.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Between Numpy Arrays and Tensors\n",
        "\n",
        "PyTorch provides a convenient interface for converting between NumPy arrays and tensors, enabling effortless integration with existing computational workflows.\n",
        "\n"
      ],
      "metadata": {
        "id": "MDK5aQLKoDBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Numpy array of Zeros.\n",
        "#This code snippet uses the NumPy library to create a NumPy array filled with zeros. However, the code to create the array is missing from the cell.\n",
        "\n",
        "np_array = np.zeros((6,6))\n",
        "np_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1bOv8M7oin7",
        "outputId": "00732b19-c0e9-41be-8aa2-a10319fefc0f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert from Numpy array to PyTorch tensor\n",
        "tensor = torch.from_numpy(np_array)\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptDWGrJco7hD",
        "outputId": "19c80e6e-ff23-47cb-b29b-7ce1c3d4697b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tensor back to Numpy array\n",
        "numpy_back = tensor.numpy ()\n",
        "numpy_back"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC6Qv9MCpQZs",
        "outputId": "a65d6457-b002-4124-971d-28b063331cc3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Mathematics and Operation Comparison\n",
        "\n",
        "This section utilizes math operations with PyTorch Tensors.\n",
        "\n",
        "1. **Addition & Subtraction-** Adds (+) and Subtracts (-) two tensor elements.\n",
        "2. **Division-** Uses the division (-)operator for precise results.\n",
        "3. **Exponentiation-** Utilises power operation (pow **).\n",
        "4. **Boolean-** Utilises boolean operators like (<,>,=) to return boolean results.\n",
        "5. **Dot Product-** Computes the sum of the element-wise product of two tensors, returning a scalar value.\n",
        "6. **Inplace Operations-** Modifies a tensor directly without creating a new one.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLEeid5hpmlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1,3,5,7,9])\n",
        "y = torch.tensor([2,4,6,8,10])\n",
        "\n",
        "#Addition- Adds two tensors\n",
        "z=torch.add(x,y)\n",
        "z1=x+y\n",
        "print(z)\n",
        "print(z1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JZydwkPtGJQ",
        "outputId": "a6a6a671-0e01-4ec5-8f5d-ee217affb498"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 3,  7, 11, 15, 19])\n",
            "tensor([ 3,  7, 11, 15, 19])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Subtraction- Performs a subtraction operation on two tensors\n",
        "z2=torch.subtract(x,y)\n",
        "z3= x- y\n",
        "print(z2)\n",
        "print(z3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yckMKqLYtpfX",
        "outputId": "ba3293bc-006a-4a98-efa7-6d9f1377a3c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1, -1, -1, -1, -1])\n",
            "tensor([-1, -1, -1, -1, -1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Division- Perform a division of two tensors\n",
        "z4=torch.divide(x,y)\n",
        "z5= x// y\n",
        "print(z4)\n",
        "print(z5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o-igBZquLhA",
        "outputId": "1d9fb043-3849-4993-e13a-115f3f262524"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5000, 0.7500, 0.8333, 0.8750, 0.9000])\n",
            "tensor([0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exponentiation#\n",
        "z6=torch.pow(x,y)\n",
        "z7=x**y\n",
        "print(z6)\n",
        "print(z7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CIqTNvVvXtk",
        "outputId": "dda68e30-24e8-43b4-868a-847a2bf7b2ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([         1,         81,      15625,    5764801, 3486784401])\n",
            "tensor([         1,         81,      15625,    5764801, 3486784401])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Boolean Operations\n",
        "z8 =x > 0\n",
        "x\n",
        "\n",
        "z9 = x<0\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTWIfbrhH6Ou",
        "outputId": "cd88f52d-1be3-4e38-ab44-eb9fbef40f2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 3, 5, 7, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dot Product\n",
        "zDot =torch.dot(x,y)\n",
        "zDot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_SXa1UkIhlj",
        "outputId": "78c4f49f-4596-4219-b4c0-bb79df68bb95"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(190)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inplace operations\n",
        "t = torch.ones(5)\n",
        "print(\"Before inplace addition:\", t)\n",
        "t.add_(x)\n",
        "print(\"After inplace addition:\", t)\n",
        "t += x  # Another inplace addition (note: t = t + x creates a new tensor)\n",
        "print(\"After second inplace addition:\", t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lccDV2BwJjex",
        "outputId": "e034f6ac-c156-4a62-bf6e-2a90579c81e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before inplace addition: tensor([1., 1., 1., 1., 1.])\n",
            "After inplace addition: tensor([ 2.,  4.,  6.,  8., 10.])\n",
            "After second inplace addition: tensor([ 3.,  7., 11., 15., 19.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Multiplication and Batch Operations\n",
        "\n",
        "Matrix operations are at the heart of deep learning. Let's find out different ways to perform multiplication.\n",
        "\n",
        "1. **Matrix Multiplication:** Uses `@` or `torch.mm()` to perform standard matrix multiplication.  \n",
        "2. **Matrix Exponentiation:** Raises a square matrix to a power using `matrix_power(n)`.\n",
        "3. **Element-wise Multiplication:** Uses `torch.mul()` or `*` for element-wise multiplication.  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r66zka5gK1J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Matrix Multiplication and Batch Operations\n",
        "\n",
        "### Example Code\n",
        "# Define two matrices\n",
        "A = torch.tensor([[1, 2], [3, 4]])\n",
        "B = torch.tensor([[5, 6, 9], [7, 8, 1]])\n",
        "\n",
        "# Matrix Multiplication\n",
        "C = A @ B\n",
        "print(\"Matrix Multiplication: \\n\", C)\n",
        "\n",
        "C = torch.mm(A, B)\n",
        "print(\"Matrix Multiplication: \\n\", C)\n",
        "\n",
        "\n",
        "# Matrix Exponentiation\n",
        "D = torch.linalg.matrix_power(A, 2)\n",
        "print(\"Matrix Exponentiation: \\n\", D)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw7bPFFDLGws",
        "outputId": "35776e35-21d3-4848-fa6b-a032dde8b792"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Multiplication: \n",
            " tensor([[19, 22, 11],\n",
            "        [43, 50, 31]])\n",
            "Matrix Multiplication: \n",
            " tensor([[19, 22, 11],\n",
            "        [43, 50, 31]])\n",
            "Matrix Exponentiation: \n",
            " tensor([[ 7, 10],\n",
            "        [15, 22]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Element-wise Multiplication\n",
        "A = torch.tensor([[1, 2], [3, 4]])\n",
        "B = torch.tensor([[5, 6], [7, 8]])\n",
        "E = torch.mul(A, B)\n",
        "print(\"Element-wise Multiplication: \\n\", E)\n",
        "# #Alternative option\n",
        "E = A * B\n",
        "print(\"Element-wise Multiplication: \\n\", E)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doja2GYDRnzl",
        "outputId": "d3e55314-ef7f-4f56-adad-786f8f41ac5e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element-wise Multiplication: \n",
            " tensor([[ 5, 12],\n",
            "        [21, 32]])\n",
            "Element-wise Multiplication: \n",
            " tensor([[ 5, 12],\n",
            "        [21, 32]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Batch Matrix Multiplication**:\n",
        "1. Batch Matrix Multiplication: Uses torch.bmm() to perform batch matrix multiplication.\n",
        "2. Batch Element-wise Multiplication: Uses torch.mul() or * for batch element-wise multiplication.\n",
        "\n"
      ],
      "metadata": {
        "id": "fSS286QXL_py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two batches of matrices\n",
        "A_batch = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "B_batch = torch.tensor([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n",
        "# Batch Matrix Multiplication\n",
        "C_batch = torch.bmm(A_batch, B_batch)\n",
        "print(\"Batch Matrix Multiplication: \\n\", C_batch)\n",
        "# Batch Element-wise Multiplication\n",
        "E_batch = torch.mul(A_batch, B_batch)\n",
        "print(\"Batch Element-wise Multiplication: \\n\", E_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo5TCCCHL9MJ",
        "outputId": "dd8e1237-a3d1-492f-b713-5057260dce07"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Matrix Multiplication: \n",
            " tensor([[[ 31,  34],\n",
            "         [ 71,  78]],\n",
            "\n",
            "        [[155, 166],\n",
            "         [211, 226]]])\n",
            "Batch Element-wise Multiplication: \n",
            " tensor([[[  9,  20],\n",
            "         [ 33,  48]],\n",
            "\n",
            "        [[ 65,  84],\n",
            "         [105, 128]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Another example of batch matrix application\n",
        "batch = 98\n",
        "x, y, z = 17, 32, 55\n",
        "tensor1 = torch.rand(batch, x, y)\n",
        "tensor2 = torch.rand(batch, y, z)\n",
        "out = torch.bmm(tensor1, tensor2) # so=hows the shape of the tensor\n",
        "print(\"Batch Matrix (first batch):\\n\", out[0])\n",
        "print(\"The shape of the batch multiplication results in:\\n\", out.shape) # Prints out the shape of tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut0RclU8SXov",
        "outputId": "9eae85a7-1f05-4ff9-d640-56302821491a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Matrix (first batch):\n",
            " tensor([[ 7.9254,  8.2898,  8.8852,  8.5008,  9.5825,  7.8063,  9.5218,  7.7836,\n",
            "          6.6981,  8.4224,  7.6196,  9.4367,  9.4317,  9.8519,  9.6635,  9.2708,\n",
            "          8.1381, 10.0274,  8.5507,  8.9452,  8.1529,  6.5712,  8.8871,  8.3546,\n",
            "          7.2615,  9.1037,  8.7985,  8.2919, 10.7117,  6.9423,  9.5739, 10.0594,\n",
            "          7.5436,  8.6067,  8.2466,  8.5262,  8.1657,  7.6389,  8.7156,  7.5846,\n",
            "          6.5544,  8.7840,  8.4811,  8.1190,  7.0277,  9.9651,  8.3882,  6.8276,\n",
            "          8.8472, 10.3327, 10.2817,  9.5997,  7.1074,  8.8534,  8.9884],\n",
            "        [ 9.0653,  9.1128,  9.0702,  9.6404, 10.4345,  8.7031, 11.4447,  8.8373,\n",
            "          8.2881,  9.7086,  7.5609,  9.5735,  9.2124,  9.4276,  9.9335,  9.1532,\n",
            "          7.7927,  9.3934,  9.6356,  8.9065,  9.3138,  8.2064,  9.5772,  8.1508,\n",
            "          7.1188,  8.6680,  9.7052,  8.6118, 11.6432,  8.2899,  9.3980, 10.5476,\n",
            "          7.3169,  7.9121,  9.0234,  9.0174,  8.1514,  8.3660,  7.8828,  8.3493,\n",
            "          7.4841,  9.0469,  7.5074,  7.6434,  8.8353,  9.4076,  9.0202,  6.6959,\n",
            "          9.6283,  9.6750,  9.2324, 10.1560,  7.8366,  9.2555,  9.3565],\n",
            "        [ 7.9748,  9.1807,  8.5926,  8.5350,  9.9692,  7.6434,  9.9339,  8.4806,\n",
            "          7.1403,  7.8435,  6.0002,  8.9473,  9.0893,  9.3234,  9.3181,  8.9719,\n",
            "          8.2498,  9.0383,  8.3312,  8.0792,  7.4596,  7.3995,  7.7880,  7.3316,\n",
            "          6.5675,  8.8550,  8.3771,  7.0441,  9.9290,  6.9087,  8.9057,  9.6898,\n",
            "          6.4377,  8.3796,  7.6143,  6.8770,  7.2690,  7.3349,  7.9342,  6.7963,\n",
            "          6.7147,  7.6970,  8.5987,  7.3657,  7.6996,  8.7730,  8.5836,  6.2784,\n",
            "          7.2228,  8.6088,  9.1096,  9.5304,  7.2034,  8.5387,  8.0312],\n",
            "        [ 7.6938,  7.8366,  8.8437,  8.2544,  9.3697,  7.0711, 10.1165,  7.7760,\n",
            "          7.1532,  6.9743,  7.0232,  7.8092,  8.7414,  8.8379,  8.7364,  7.3644,\n",
            "          8.3291,  8.7127,  8.3309,  8.3492,  7.8515,  6.8595,  8.7987,  7.3324,\n",
            "          6.5465,  7.6923,  8.5225,  7.7594,  9.8863,  6.8983,  7.8171,  8.6347,\n",
            "          7.0727,  7.2972,  7.9254,  7.6036,  6.7144,  7.5536,  7.5528,  6.3392,\n",
            "          6.6193,  8.2129,  8.5407,  7.3272,  6.6780,  8.1394,  7.5790,  6.6827,\n",
            "          8.2592,  8.5419, 10.0503,  9.3265,  7.1139,  7.6413,  8.5914],\n",
            "        [ 8.7732,  9.3677,  9.0673,  9.4617,  9.8928,  8.5167, 10.6132,  8.4230,\n",
            "          7.6702,  9.0260,  6.3067,  9.2841,  8.3169,  9.9787,  9.4348,  8.8327,\n",
            "          7.3452,  9.2429,  9.0313,  8.2076,  8.2522,  7.8929,  9.5134,  7.1462,\n",
            "          6.3804,  9.3916,  8.7712,  8.9231, 10.0587,  7.3167,  9.0441, 10.5400,\n",
            "          6.6784,  8.5874,  8.9934,  8.1360,  7.4226,  6.7961,  7.9090,  8.5471,\n",
            "          7.6123,  8.4720,  8.3236,  6.9248,  8.9123,  9.9692,  9.4813,  5.2545,\n",
            "          9.2200,  9.3516,  9.2239,  9.6792,  7.4676,  8.2791,  8.2374],\n",
            "        [ 7.1646,  8.1268,  8.4007,  8.2058,  8.6587,  7.8320, 10.8263,  7.7365,\n",
            "          7.1388,  7.5750,  6.8053,  8.2920,  8.2263,  8.8016,  9.7104,  7.7732,\n",
            "          7.7518,  8.3561,  8.1777,  9.4016,  6.8311,  6.2793,  8.5745,  6.7892,\n",
            "          7.5170,  8.7641,  7.5904,  7.5007,  8.9676,  7.5577,  8.1802,  9.9679,\n",
            "          6.5209,  8.4826,  7.9560,  7.8991,  7.3965,  7.2355,  8.4621,  7.0652,\n",
            "          5.9418,  7.4602,  8.2376,  7.1817,  7.0534,  8.1834,  8.4615,  6.5782,\n",
            "          9.2475,  9.5424,  9.7211, 10.1684,  6.9079,  8.0306,  8.3227],\n",
            "        [ 7.2441,  8.1912,  7.8851,  7.8482,  8.0739,  6.8319,  9.6614,  6.7973,\n",
            "          6.6677,  6.5388,  5.8009,  6.8075,  7.3219,  7.6886,  8.5100,  7.3544,\n",
            "          7.1187,  7.7644,  8.1703,  8.2600,  7.4287,  6.2110,  7.8075,  6.2379,\n",
            "          6.0750,  7.6830,  7.3838,  7.0427,  9.0488,  7.1345,  7.5304,  8.2651,\n",
            "          6.1004,  7.4562,  7.0668,  6.0980,  7.1145,  5.9371,  7.0460,  6.5385,\n",
            "          6.3099,  6.4581,  6.9535,  7.2450,  6.9050,  7.4972,  7.2755,  5.4426,\n",
            "          7.4854,  7.4149,  8.8260,  9.2417,  6.2343,  7.3929,  7.7406],\n",
            "        [ 6.7654,  7.7690,  9.0905,  9.0370,  9.3576,  7.7257,  9.3676,  7.3101,\n",
            "          7.1674,  7.4843,  6.7769,  7.6885,  8.5287,  9.0136,  8.4250,  7.3918,\n",
            "          8.2916,  7.9881,  7.4393,  8.6193,  7.4531,  6.2321,  7.6739,  7.2805,\n",
            "          6.7524,  8.8020,  7.8533,  8.4333, 10.0377,  6.5531,  7.7955,  8.9221,\n",
            "          7.3010,  7.7648,  8.5945,  8.4540,  8.1320,  6.8257,  7.9598,  6.6643,\n",
            "          6.5673,  8.1377,  8.3851,  7.5345,  7.1220,  8.6567,  7.9196,  6.4305,\n",
            "          8.4368,  8.4324, 10.0320, 10.0349,  7.1749,  6.9120,  7.9190],\n",
            "        [ 7.5579,  8.5533,  7.0498,  7.2764,  8.0864,  6.5142,  8.3179,  7.0561,\n",
            "          5.4944,  6.8822,  5.2508,  8.2385,  7.1254,  8.8630,  8.1033,  8.0544,\n",
            "          6.7368,  7.9923,  7.2914,  6.3603,  6.9871,  6.8639,  7.3567,  7.2917,\n",
            "          5.6000,  7.9735,  8.6053,  6.0491,  8.6732,  5.1560,  7.1123,  9.1024,\n",
            "          5.2419,  7.5136,  7.7016,  5.6077,  6.1241,  6.4737,  5.7050,  6.3868,\n",
            "          6.0634,  6.9400,  7.8074,  7.2979,  6.5183,  8.2864,  8.4717,  4.6787,\n",
            "          7.1925,  8.1920,  7.6747,  7.9753,  6.3300,  6.5525,  7.1902],\n",
            "        [ 9.9174, 10.6351, 10.2852, 10.2251, 11.6174,  9.1575, 12.0245,  9.4407,\n",
            "          8.1796, 10.0288,  7.1381, 10.2298, 10.5089, 10.2784, 11.2648,  9.8186,\n",
            "          8.8283, 10.1732,  9.8976,  9.6023,  9.0013,  9.4766,  9.2257,  8.7475,\n",
            "          8.5209, 10.4543, 10.4537,  8.8783, 11.5694,  8.1055, 10.2216, 11.7209,\n",
            "          8.2305,  9.8101,  9.4819,  8.7718,  8.0849,  8.1536,  8.8721,  9.3527,\n",
            "          8.8711,  9.5522,  9.7093,  9.0764,  8.8876,  9.8730, 10.9368,  6.9111,\n",
            "         10.0622, 11.0077, 10.1231, 11.6193,  8.6716,  9.5993, 10.1601],\n",
            "        [ 7.3582,  8.2833,  8.3370,  8.6610,  9.1450,  7.4663,  9.2721,  7.2414,\n",
            "          6.6280,  7.7174,  5.8337,  7.2413,  7.3446,  8.6920,  8.5028,  7.8841,\n",
            "          6.6098,  7.0568,  7.7939,  8.3563,  6.8093,  6.9727,  7.6607,  6.0564,\n",
            "          6.2179,  8.5199,  7.3075,  8.1944,  9.5370,  6.2691,  8.1629,  8.6432,\n",
            "          6.4189,  8.2293,  8.2763,  6.8835,  5.7618,  5.5941,  7.3571,  7.3413,\n",
            "          7.0969,  7.4483,  7.0140,  6.8729,  7.2287,  8.4128,  9.1042,  4.9464,\n",
            "          8.9189,  8.3113,  8.7358,  9.5282,  7.2529,  6.7728,  7.4975],\n",
            "        [ 7.4800,  7.0545,  7.3352,  7.4610,  8.2201,  6.0370,  8.3044,  6.1312,\n",
            "          5.8177,  7.5568,  5.9020,  7.5783,  7.1398,  7.6251,  7.2141,  7.7972,\n",
            "          6.4929,  7.5119,  7.1721,  6.5242,  6.6523,  6.6516,  6.9313,  6.5485,\n",
            "          5.2226,  7.9056,  7.8818,  6.5576,  8.7649,  5.4572,  6.7324,  8.3331,\n",
            "          5.5456,  6.7418,  6.6021,  5.9139,  5.7202,  6.0735,  6.2091,  6.2933,\n",
            "          6.0339,  6.3467,  6.8261,  6.3157,  6.1028,  7.5127,  7.7886,  4.1434,\n",
            "          7.1285,  7.0112,  7.6553,  7.5425,  6.1717,  5.9577,  6.3821],\n",
            "        [ 6.1926,  7.0473,  7.2461,  7.5157,  8.2063,  6.9058,  8.4426,  7.4437,\n",
            "          6.0894,  6.8964,  5.3845,  6.5076,  6.6903,  8.1046,  7.0550,  6.8819,\n",
            "          6.0656,  6.4630,  6.8943,  6.7293,  5.8309,  5.6964,  7.6111,  5.4004,\n",
            "          5.4925,  6.6008,  6.4464,  6.5413,  8.7858,  6.1527,  6.7588,  7.5182,\n",
            "          5.5699,  7.0547,  6.7192,  6.5919,  5.7365,  5.2310,  6.7455,  6.0198,\n",
            "          6.0623,  7.2084,  6.2108,  5.1530,  6.5190,  7.8376,  7.5627,  5.0898,\n",
            "          7.3635,  7.6428,  7.9607,  8.2331,  6.7056,  7.1990,  6.9049],\n",
            "        [ 9.0010,  9.2618,  8.7746,  8.4486, 10.2566,  7.0367, 10.3958,  8.1333,\n",
            "          7.1059,  8.6286,  7.7996,  8.9105,  9.5643,  9.9963,  9.8831,  8.8782,\n",
            "          8.3176,  9.5255,  8.8675,  9.3438,  8.5601,  8.0903,  9.5823,  8.1380,\n",
            "          7.9117,  9.0370,  8.7860,  7.1106, 10.3613,  7.0347,  8.8106, 10.5689,\n",
            "          7.0417,  7.5936,  8.2205,  7.5840,  7.0051,  7.0955,  8.5633,  7.9997,\n",
            "          6.5651,  8.5276,  9.3872,  8.5108,  8.3568,  9.0595,  9.2675,  6.6571,\n",
            "          9.2483, 10.1770, 10.4212, 11.3106,  7.8809,  8.5559,  9.3397],\n",
            "        [ 7.1633,  7.2701,  7.4101,  8.1311,  8.8817,  7.7627,  8.5308,  7.3538,\n",
            "          5.7254,  6.6462,  5.2985,  8.8168,  8.4115,  8.1736,  8.0840,  7.7340,\n",
            "          6.9958,  8.4245,  8.1017,  6.4635,  7.6126,  6.2364,  8.0482,  6.9259,\n",
            "          6.6223,  7.5996,  8.0557,  6.9330,  8.8476,  6.2936,  7.8020,  8.3204,\n",
            "          6.4612,  8.3817,  7.3735,  6.7055,  7.5805,  5.7872,  7.7265,  6.6531,\n",
            "          6.3313,  7.6436,  7.5746,  7.2476,  5.9138,  8.2364,  7.4223,  5.8430,\n",
            "          7.4942,  7.5401,  7.9784,  9.0142,  6.6573,  7.6973,  7.5752],\n",
            "        [10.0278, 10.9029,  9.5063,  9.9696, 11.0619,  8.1444, 13.0890,  9.8316,\n",
            "          8.2443,  9.9996,  8.2405, 10.2173,  9.7623, 11.0458, 11.8454, 10.6569,\n",
            "          9.2286, 10.5165, 10.3787,  9.5446,  9.4051,  8.7847, 10.4202,  8.6501,\n",
            "          8.3338, 10.1606, 10.5516,  9.2568, 11.9503,  7.5101,  9.9151, 11.5128,\n",
            "          7.9390, 10.1540, 10.1776,  8.8267,  8.5202,  8.5023,  8.8445,  8.5706,\n",
            "          7.6316,  8.7118, 10.3146,  9.9080,  9.4861, 10.5717, 11.0454,  6.5521,\n",
            "         10.3869, 10.8954, 10.6444, 11.5491,  8.5830,  9.1201,  9.5368],\n",
            "        [ 5.9889,  6.1791,  5.5680,  5.9299,  6.1637,  5.3776,  7.0109,  5.3897,\n",
            "          4.8872,  5.2769,  5.1538,  7.0338,  6.8659,  6.9235,  7.6470,  6.2213,\n",
            "          6.7618,  7.1654,  5.2722,  6.0334,  5.4657,  4.9567,  5.8604,  6.0552,\n",
            "          4.6438,  6.9589,  6.1893,  5.8406,  6.8631,  4.8882,  6.5023,  7.3843,\n",
            "          5.2260,  6.6074,  5.6285,  5.9881,  6.0689,  5.8404,  5.7982,  4.2295,\n",
            "          4.5384,  5.2855,  6.9034,  5.8618,  5.0588,  6.0337,  5.5248,  5.3179,\n",
            "          5.9258,  6.7726,  6.7197,  6.8068,  5.3075,  5.6755,  5.2292]])\n",
            "The shape of the batch multiplication results in:\n",
            " torch.Size([98, 17, 55])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Broadcasting and Other Useful Operations**\n",
        "\n",
        "> Broadcasting allows arithmetic operations on tensors of different shapes. This section also demonstrates additional useful functions.\n",
        "\n",
        "- **Broadcasting:** Automatically expands smaller tensors to match larger ones in operations.  \n",
        "- **Summation:** `torch.sum(x, dim=0)` computes sum along a specific dimension.  \n",
        "- **Min/Max Values:** `torch.max()` and `torch.min()` return the highest and lowest values along a dimension.  \n",
        "- **Absolute Values:** `torch.abs(x)` gets the element-wise absolute values.  \n",
        "- **Argmax/Argmin:** `torch.argmax()` and `torch.argmin()` return the index of max/min values.  \n",
        "- **Mean Calculation:** `torch.mean(x.float(), dim=0)` computes the mean (ensuring float dtype).  \n",
        "- **Element-wise Comparison:** `torch.eq(x, y)` checks equality between two tensors.  \n",
        "- **Sorting:** `torch.sort(y, dim=0)` sorts tensor elements and returns indices.  \n",
        "- **Clamping:** `torch.clamp(x, min=0)` restricts values within a range.  \n",
        "- **Boolean Operations:** `torch.any(x_bool)` checks if any value is `True`, `torch.all(x_bool)` checks if all are `True`."
      ],
      "metadata": {
        "id": "1Lg9-AW7U5vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tensors with descriptive names\n",
        "\n",
        "matrix = torch.rand(5, 5)\n",
        "vector = torch.rand(5)\n",
        "\n",
        "# Print tensors with clear labels\n",
        "print(\"Matrix (5x5):\\n\", matrix)\n",
        "print(\"Vector (5):\\n\", vector)\n",
        "\n",
        "# Perform broadcasting operations\n",
        "broadcasted_subtraction = matrix - vector\n",
        "broadcasted_power = matrix ** vector\n",
        "\n",
        "# Print results with descriptive labels\n",
        "print(\"Matrix - Vector (broadcasted subtraction):\\n\", broadcasted_subtraction)\n",
        "print(\"Matrix raised to the power of Vector (broadcasted power):\\n\", broadcasted_power)\n",
        "\n",
        "# Summation of tensor elements along a dimension 0\n",
        "sum_along_dim0 = torch.sum(matrix,dim=0)\n",
        "print(\"Sum along dimension 0:\\n\", sum_along_dim0)\n",
        "\n",
        "# Determining the Maximum and **Mininum** values of a tensor with 0-dimension\n",
        "value_max, index_max = torch.max(matrix, dim = 0)\n",
        "value_min, index_min = torch.min(matrix, dim =0)\n",
        "print(\"Maximum Values (along dimension 0):\\n\", value_max)\n",
        "print(\"Indices of Maximum Values (along dimension 0):\\n\", index_max)\n",
        "\n",
        "print(\"Minimum Values (along dimension 0):\\n\", value_min)\n",
        "print(\"Indices of Minimum Values (along dimension 0):\\n\", index_min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6ApwxmQU4s3",
        "outputId": "c26955c1-3b67-4518-a7c1-84d132ee95e8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix (5x5):\n",
            " tensor([[0.9981, 0.7560, 0.8607, 0.5471, 0.6866],\n",
            "        [0.3240, 0.9405, 0.0533, 0.6738, 0.7865],\n",
            "        [0.5608, 0.4181, 0.3402, 0.2968, 0.4588],\n",
            "        [0.0207, 0.1091, 0.0662, 0.2091, 0.6319],\n",
            "        [0.6682, 0.1788, 0.1994, 0.0287, 0.4932]])\n",
            "Vector (5):\n",
            " tensor([0.2968, 0.7401, 0.6726, 0.5835, 0.8406])\n",
            "Matrix - Vector (broadcasted subtraction):\n",
            " tensor([[ 0.7013,  0.0159,  0.1880, -0.0364, -0.1540],\n",
            "        [ 0.0272,  0.2004, -0.6193,  0.0903, -0.0541],\n",
            "        [ 0.2640, -0.3220, -0.3325, -0.2867, -0.3818],\n",
            "        [-0.2761, -0.6310, -0.6064, -0.3744, -0.2087],\n",
            "        [ 0.3714, -0.5614, -0.4733, -0.5548, -0.3474]])\n",
            "Matrix raised to the power of Vector (broadcasted power):\n",
            " tensor([[0.9994, 0.8130, 0.9040, 0.7033, 0.7290],\n",
            "        [0.7157, 0.9556, 0.1392, 0.7942, 0.8172],\n",
            "        [0.8423, 0.5244, 0.4842, 0.4922, 0.5195],\n",
            "        [0.3163, 0.1941, 0.1611, 0.4013, 0.6798],\n",
            "        [0.8872, 0.2797, 0.3380, 0.1259, 0.5520]])\n",
            "Sum along dimension 0:\n",
            " tensor([2.5718, 2.4026, 1.5198, 1.7554, 3.0570])\n",
            "Maximum Values (along dimension 0):\n",
            " tensor([0.9981, 0.9405, 0.8607, 0.6738, 0.7865])\n",
            "Indices of Maximum Values (along dimension 0):\n",
            " tensor([0, 1, 0, 1, 1])\n",
            "Minimum Values (along dimension 0):\n",
            " tensor([0.0207, 0.1091, 0.0533, 0.0287, 0.4588])\n",
            "Indices of Minimum Values (along dimension 0):\n",
            " tensor([3, 3, 1, 4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tensors x and y\n",
        "x = torch.tensor([1,5,6])  # Replace with actual values\n",
        "y = torch.tensor([7,9,2])  # Replace with actual values\n",
        "# Compute and print various tensor operations\n",
        "print(\"Absolute Values:\")\n",
        "print(torch.abs(x))\n",
        "print(\"\\nArgmax (along dimension 0):\")\n",
        "print(torch.argmax(x, dim=0))\n",
        "print(\"\\nArgmin (along dimension 0):\")\n",
        "print(torch.argmin(x, dim=0))\n",
        "print(\"\\nMean (converted to float, along dimension 0):\")\n",
        "print(torch.mean(x.float(), dim=0))\n",
        "print(\"\\nElement-wise Equality (x == y):\")\n",
        "print(torch.eq(x, y))\n",
        "\n",
        "sorted_y, indices = torch.sort(y, dim=0, descending=False)\n",
        "print(\"Sorted y and indices:\", sorted_y, indices)\n",
        "\n",
        "# Sorting\n",
        "top_k_values, top_k_indices = torch.topk(x, k=3, dim=0)\n",
        "print(\"Top-k values and indices:\", top_k_values, top_k_indices)\n",
        "\n",
        "# Clamp values to a minimum of 0\n",
        "clamped_x = torch.clamp(x, min=0)\n",
        "print(f\"Clamped x: {clamped_x}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToinYg6naIVe",
        "outputId": "370c9483-bcb8-4612-8e07-a78f5b01c224"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolute Values:\n",
            "tensor([1, 5, 6])\n",
            "\n",
            "Argmax (along dimension 0):\n",
            "tensor(2)\n",
            "\n",
            "Argmin (along dimension 0):\n",
            "tensor(0)\n",
            "\n",
            "Mean (converted to float, along dimension 0):\n",
            "tensor(4.)\n",
            "\n",
            "Element-wise Equality (x == y):\n",
            "tensor([False, False, False])\n",
            "Sorted y and indices: tensor([2, 7, 9]) tensor([2, 0, 1])\n",
            "Top-k values and indices: tensor([6, 5, 1]) tensor([2, 1, 0])\n",
            "Clamped x: tensor([1, 5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a boolean tensor\n",
        "x_bool = torch.tensor([1, 0, 1, 1, 1], dtype=torch.bool)\n",
        "# Perform boolean operations\n",
        "any_true = torch.any(x_bool)\n",
        "all_true = torch.all(x_bool)\n",
        "# Print results\n",
        "print(f\"Any True: {any_true}\")\n",
        "print(f\"All True: {all_true}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axm6q9gPW4gk",
        "outputId": "ee22c052-d987-4de2-aad6-6546eef5c330"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any True: True\n",
            "All True: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tensor Indexing**\n",
        "\n",
        "> Access and modify tensor elements using indexing, slicing, and advanced indexing techniques.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Accessing Rows and Columns\n",
        "\n",
        "*   Rows: Use x[row, :] to access a specific row.\n",
        "*   Columns: Use x[:, col] to access a specific column.\n",
        "\n",
        "## Slicing\n",
        "Row Slicing: x[row, start:end] extracts a portion of a row.\n",
        "\n",
        "## Modifying Elements\n",
        "Direct Assignment: Assign values directly using x[row, col] = value.\n",
        "\n",
        "# Advanced Indexing Techniques\n",
        "- **Fancy Indexing:** Use a list of indices to select multiple elements at once.\n",
        "- **Conditional Indexing**: Extract elements using conditions like (x < 2) | (x > 8).\n",
        "- **Finding Even Numbers:** Use x.remainder(2) == 0 to filter even values.\n",
        "\n",
        "#Conditional Selection with torch.where()\n",
        "- **Conditional Choice:** Use torch.where() to choose values based on a condition."
      ],
      "metadata": {
        "id": "_d6RzkeOc2w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a random tensor with shape (batch size and reatures)\n",
        "batch_size = 50\n",
        "features = 10\n",
        "x = torch.rand(batch_size, features)\n",
        "\n",
        "#Print the first row\n",
        "print(\"This is the first row of the tensor:\\n\", x[0,:])\n",
        "\n",
        "#Print the second row\n",
        "print(\"This is the second row of the tensor:\\n\", x[1,:])\n",
        "\n",
        "\n",
        "#Print the first column\n",
        "print(\"This is the first row of the tensor:\\n\", x[:,0])\n",
        "\n",
        "#Output the second column\n",
        "print(\"This is the second column of the tensor:\\n\", x[:,1])\n",
        "\n",
        "#Print the first 10 elements of the third row\n",
        "print(\"This is the first 10 elements of the third row:\\n\", x[2,0:10])\n",
        "\n",
        "#Modify specific element  (set first element to 100)\n",
        "x[0,0]=100\n",
        "print(x)\n",
        "\n",
        "#Fancy indexing example\n",
        "x =torch.arange(70)\n",
        "indices =[8,9, 10]\n",
        "print(\"fancy indexing result\", x[indices])\n",
        "\n",
        "#Advanced indexing : select elements based on a condition\n",
        "x1 = torch.arange(70)\n",
        "print(\"Elements where x1 < 70 or x1 > 90:\",x1[(x1<70) | (x1 > 90)])\n",
        "print(\"Even numbers in x1:\", x1[x1.remainder(2)==0])\n",
        "\n",
        "#Using torch  to select values based on a condition\n",
        "print(\"Using torch.where:\", torch.where(x1 > 7, x1 * 2, x1))\n",
        "\n",
        "#selection option 2\n",
        "x = torch.tensor([1, 2, 3, 4, 5])\n",
        "y = torch.tensor([10, 20, 30, 40, 50])\n",
        "condition = torch.tensor([True, False, True, False, True])\n",
        "selected_values = torch.where(condition, x, y)\n",
        "print(selected_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk5-UieQdcyv",
        "outputId": "3d08c2e4-6f9e-4013-d52b-655b9dc467b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first row of the tensor:\n",
            " tensor([0.6095, 0.8770, 0.3208, 0.3880, 0.6311, 0.8887, 0.6308, 0.9437, 0.1765,\n",
            "        0.1308])\n",
            "This is the second row of the tensor:\n",
            " tensor([0.9535, 0.4668, 0.2816, 0.6225, 0.0927, 0.1929, 0.8161, 0.2866, 0.6681,\n",
            "        0.9034])\n",
            "This is the first row of the tensor:\n",
            " tensor([0.6095, 0.9535, 0.0645, 0.4010, 0.3723, 0.1730, 0.9011, 0.5818, 0.0469,\n",
            "        0.2166, 0.0636, 0.7635, 0.9885, 0.5974, 0.2896, 0.5804, 0.6550, 0.5179,\n",
            "        0.3225, 0.0704, 0.6223, 0.0456, 0.8093, 0.3715, 0.9279, 0.7162, 0.4381,\n",
            "        0.3982, 0.9193, 0.2314, 0.3837, 0.4606, 0.8543, 0.1102, 0.3958, 0.9994,\n",
            "        0.9273, 0.7551, 0.4869, 0.8329, 0.6730, 0.7639, 0.5675, 0.5939, 0.9000,\n",
            "        0.0152, 0.6001, 0.1936, 0.5897, 0.5686])\n",
            "This is the second column of the tensor:\n",
            " tensor([0.8770, 0.4668, 0.7543, 0.6632, 0.1032, 0.3634, 0.9881, 0.2540, 0.8268,\n",
            "        0.2256, 0.7529, 0.9086, 0.2848, 0.6988, 0.1978, 0.1997, 0.6152, 0.7463,\n",
            "        0.3620, 0.0126, 0.4674, 0.2444, 0.4043, 0.4806, 0.1555, 0.4854, 0.9614,\n",
            "        0.7944, 0.9030, 0.6918, 0.6004, 0.9372, 0.1566, 0.9491, 0.3266, 0.6263,\n",
            "        0.4869, 0.4042, 0.2186, 0.4755, 0.3575, 0.9591, 0.7705, 0.4353, 0.9693,\n",
            "        0.7609, 0.7618, 0.4298, 0.1139, 0.4496])\n",
            "This is the first 10 elements of the third row:\n",
            " tensor([0.0645, 0.7543, 0.6531, 0.8498, 0.5533, 0.9087, 0.4180, 0.3528, 0.8692,\n",
            "        0.4098])\n",
            "tensor([[1.0000e+02, 8.7704e-01, 3.2077e-01, 3.8801e-01, 6.3108e-01, 8.8867e-01,\n",
            "         6.3084e-01, 9.4369e-01, 1.7649e-01, 1.3084e-01],\n",
            "        [9.5353e-01, 4.6676e-01, 2.8156e-01, 6.2254e-01, 9.2732e-02, 1.9288e-01,\n",
            "         8.1613e-01, 2.8664e-01, 6.6812e-01, 9.0340e-01],\n",
            "        [6.4454e-02, 7.5435e-01, 6.5310e-01, 8.4983e-01, 5.5329e-01, 9.0868e-01,\n",
            "         4.1799e-01, 3.5285e-01, 8.6923e-01, 4.0977e-01],\n",
            "        [4.0098e-01, 6.6323e-01, 2.8968e-01, 4.0870e-01, 4.1907e-01, 8.7656e-01,\n",
            "         9.2155e-01, 2.4327e-01, 5.2089e-01, 4.1320e-01],\n",
            "        [3.7231e-01, 1.0324e-01, 2.0154e-01, 3.7502e-01, 3.1753e-01, 4.7744e-01,\n",
            "         8.7440e-01, 9.5626e-01, 8.4944e-01, 2.1151e-01],\n",
            "        [1.7297e-01, 3.6345e-01, 4.5022e-01, 2.2023e-01, 4.6967e-01, 7.1930e-01,\n",
            "         1.7444e-01, 7.4612e-01, 3.6669e-01, 3.8181e-01],\n",
            "        [9.0115e-01, 9.8807e-01, 6.5995e-01, 7.9083e-01, 7.8518e-01, 2.3836e-01,\n",
            "         6.7734e-01, 4.2052e-01, 8.1116e-01, 2.2491e-01],\n",
            "        [5.8184e-01, 2.5401e-01, 1.3623e-02, 9.7229e-02, 7.6690e-01, 5.8698e-01,\n",
            "         4.4860e-01, 5.3977e-01, 4.3309e-01, 4.4953e-01],\n",
            "        [4.6857e-02, 8.2676e-01, 8.2986e-01, 7.1521e-01, 1.1106e-02, 4.7271e-01,\n",
            "         8.0361e-01, 2.7046e-01, 6.8382e-01, 1.6012e-01],\n",
            "        [2.1659e-01, 2.2561e-01, 7.7412e-01, 5.1081e-01, 7.1960e-01, 9.6346e-01,\n",
            "         2.5957e-01, 7.6269e-01, 1.3146e-01, 6.9337e-01],\n",
            "        [6.3636e-02, 7.5287e-01, 1.5742e-01, 3.6414e-01, 1.3255e-01, 8.1094e-01,\n",
            "         9.4126e-01, 6.8860e-01, 9.6808e-01, 9.3601e-01],\n",
            "        [7.6353e-01, 9.0859e-01, 1.0607e-01, 9.0490e-01, 4.7995e-01, 5.8426e-01,\n",
            "         9.3709e-02, 1.7769e-01, 5.1661e-01, 3.8549e-01],\n",
            "        [9.8855e-01, 2.8476e-01, 4.5966e-01, 8.7997e-01, 5.3849e-01, 9.3899e-01,\n",
            "         9.7504e-02, 6.5431e-01, 7.2972e-01, 5.1069e-01],\n",
            "        [5.9738e-01, 6.9881e-01, 9.5572e-01, 9.2957e-01, 3.1968e-01, 2.7549e-01,\n",
            "         2.3499e-01, 7.4170e-01, 7.9634e-01, 6.5491e-01],\n",
            "        [2.8961e-01, 1.9783e-01, 2.4177e-01, 1.0289e-01, 2.0296e-01, 6.3812e-01,\n",
            "         6.9767e-01, 5.1330e-01, 7.3734e-01, 2.9725e-01],\n",
            "        [5.8040e-01, 1.9974e-01, 1.0665e-02, 5.5916e-01, 5.6988e-01, 3.6807e-01,\n",
            "         4.9393e-01, 6.1176e-01, 7.3518e-01, 4.5505e-01],\n",
            "        [6.5497e-01, 6.1522e-01, 8.3626e-01, 1.9010e-01, 2.2537e-01, 8.4884e-01,\n",
            "         7.4766e-01, 9.1742e-01, 4.1312e-01, 3.8167e-01],\n",
            "        [5.1791e-01, 7.4631e-01, 3.3539e-01, 3.8329e-01, 7.6011e-01, 9.4869e-01,\n",
            "         5.3854e-01, 3.5969e-01, 3.4729e-01, 6.9155e-01],\n",
            "        [3.2246e-01, 3.6204e-01, 7.7686e-01, 1.4960e-01, 8.0318e-01, 7.9133e-01,\n",
            "         4.6430e-01, 7.9188e-01, 6.1365e-01, 1.3118e-01],\n",
            "        [7.0368e-02, 1.2619e-02, 8.6401e-01, 7.4922e-02, 2.0570e-01, 8.6280e-02,\n",
            "         2.5629e-01, 5.6014e-03, 1.9599e-01, 5.9532e-01],\n",
            "        [6.2229e-01, 4.6745e-01, 7.0671e-01, 1.0907e-01, 2.1120e-01, 2.8276e-02,\n",
            "         3.4808e-01, 3.9142e-01, 9.4772e-01, 1.9936e-01],\n",
            "        [4.5617e-02, 2.4439e-01, 6.0079e-01, 2.5029e-01, 7.0262e-01, 1.4534e-01,\n",
            "         9.8397e-01, 9.3410e-01, 2.7983e-01, 7.7736e-01],\n",
            "        [8.0926e-01, 4.0425e-01, 3.2267e-01, 7.3430e-01, 9.3612e-01, 2.7715e-01,\n",
            "         9.9023e-01, 8.6564e-01, 7.6982e-01, 4.7983e-01],\n",
            "        [3.7154e-01, 4.8055e-01, 4.8206e-01, 2.2616e-01, 8.2917e-01, 9.2137e-02,\n",
            "         1.8300e-01, 9.4538e-01, 3.4960e-01, 6.7912e-01],\n",
            "        [9.2786e-01, 1.5548e-01, 1.4118e-01, 3.5557e-01, 1.7601e-01, 5.5096e-01,\n",
            "         2.5114e-01, 7.5785e-01, 2.3668e-01, 4.3598e-01],\n",
            "        [7.1622e-01, 4.8538e-01, 5.8358e-01, 6.8147e-01, 8.0968e-01, 1.4072e-01,\n",
            "         8.3812e-01, 7.2144e-01, 9.9042e-01, 2.4138e-01],\n",
            "        [4.3808e-01, 9.6144e-01, 7.6157e-01, 1.3507e-01, 1.4602e-01, 9.5843e-01,\n",
            "         4.4500e-01, 4.5261e-01, 2.3264e-01, 4.9659e-01],\n",
            "        [3.9818e-01, 7.9440e-01, 6.5872e-01, 5.6795e-01, 1.4662e-01, 8.4810e-02,\n",
            "         1.6902e-01, 1.6766e-01, 8.8352e-01, 1.2488e-02],\n",
            "        [9.1934e-01, 9.0297e-01, 6.7763e-01, 1.7948e-01, 1.6387e-01, 4.2925e-01,\n",
            "         6.8838e-01, 7.0737e-01, 9.1615e-01, 9.3255e-01],\n",
            "        [2.3141e-01, 6.9182e-01, 6.1115e-02, 4.1449e-01, 1.8463e-02, 5.4659e-01,\n",
            "         3.6035e-01, 4.4110e-01, 7.9226e-02, 9.8212e-01],\n",
            "        [3.8368e-01, 6.0042e-01, 5.8001e-02, 8.3226e-01, 3.5105e-01, 1.7043e-01,\n",
            "         7.0680e-01, 1.9789e-01, 4.8584e-01, 5.3112e-01],\n",
            "        [4.6062e-01, 9.3721e-01, 1.4910e-01, 6.8335e-01, 9.6139e-01, 8.0031e-01,\n",
            "         9.8960e-01, 2.4049e-01, 9.8312e-01, 6.7279e-01],\n",
            "        [8.5427e-01, 1.5665e-01, 4.2326e-01, 3.1743e-01, 9.7540e-01, 6.1979e-01,\n",
            "         9.4522e-01, 7.3916e-02, 6.2634e-01, 2.0489e-01],\n",
            "        [1.1020e-01, 9.4908e-01, 1.2223e-01, 1.4446e-01, 6.6313e-01, 3.7333e-01,\n",
            "         2.0611e-01, 4.7882e-01, 7.1944e-01, 1.4439e-01],\n",
            "        [3.9579e-01, 3.2657e-01, 3.2636e-01, 1.6189e-01, 6.1651e-02, 6.8811e-01,\n",
            "         1.2009e-01, 3.9090e-01, 2.3101e-01, 4.9652e-01],\n",
            "        [9.9935e-01, 6.2629e-01, 5.6578e-02, 8.1108e-01, 8.6383e-01, 3.9698e-01,\n",
            "         8.6364e-01, 7.4867e-01, 2.6817e-01, 5.5816e-01],\n",
            "        [9.2729e-01, 4.8690e-01, 6.9349e-01, 3.6075e-01, 9.7795e-01, 9.7748e-01,\n",
            "         8.6682e-01, 1.9891e-01, 5.1019e-01, 2.9140e-01],\n",
            "        [7.5515e-01, 4.0421e-01, 3.7212e-01, 9.9530e-01, 3.7701e-01, 4.3478e-01,\n",
            "         8.1884e-01, 7.5390e-02, 8.5683e-01, 9.6100e-01],\n",
            "        [4.8690e-01, 2.1856e-01, 6.5248e-02, 4.5294e-01, 2.5165e-01, 5.5248e-01,\n",
            "         6.2231e-01, 1.2836e-01, 5.7220e-01, 2.1760e-01],\n",
            "        [8.3286e-01, 4.7553e-01, 9.3349e-01, 9.7884e-01, 6.9468e-01, 5.1585e-01,\n",
            "         8.3775e-01, 6.6151e-01, 6.1524e-01, 7.3268e-02],\n",
            "        [6.7297e-01, 3.5754e-01, 4.1502e-01, 7.0822e-01, 9.9285e-01, 2.0850e-01,\n",
            "         7.8645e-01, 2.6764e-01, 7.4234e-01, 6.7585e-01],\n",
            "        [7.6391e-01, 9.5913e-01, 7.0653e-01, 3.4538e-01, 4.6767e-01, 4.5502e-01,\n",
            "         5.5176e-01, 5.8105e-01, 7.0689e-01, 6.7425e-02],\n",
            "        [5.6754e-01, 7.7047e-01, 6.5110e-01, 4.8168e-01, 2.5885e-01, 4.2456e-03,\n",
            "         4.8007e-01, 3.0064e-02, 1.1410e-01, 8.6829e-01],\n",
            "        [5.9387e-01, 4.3531e-01, 3.2598e-01, 3.2367e-01, 9.1266e-01, 5.5947e-02,\n",
            "         4.7609e-01, 8.0914e-01, 1.1174e-01, 3.1173e-01],\n",
            "        [9.0005e-01, 9.6932e-01, 7.9767e-01, 7.2354e-02, 6.0545e-01, 1.9591e-01,\n",
            "         9.0766e-01, 3.1238e-01, 6.5117e-02, 4.4564e-01],\n",
            "        [1.5213e-02, 7.6089e-01, 6.0745e-01, 5.2112e-01, 8.0338e-01, 9.0380e-02,\n",
            "         4.5622e-02, 4.1773e-02, 6.4671e-01, 8.7586e-01],\n",
            "        [6.0010e-01, 7.6182e-01, 9.7045e-01, 4.8237e-01, 3.6364e-01, 5.5261e-01,\n",
            "         2.2796e-01, 9.7912e-01, 3.8957e-01, 5.8574e-02],\n",
            "        [1.9362e-01, 4.2979e-01, 7.0247e-01, 7.1874e-01, 7.2931e-01, 6.5975e-01,\n",
            "         7.4990e-02, 8.4383e-01, 8.4118e-01, 4.7422e-01],\n",
            "        [5.8973e-01, 1.1388e-01, 9.3966e-01, 3.2492e-01, 2.0070e-01, 5.8446e-01,\n",
            "         5.5007e-02, 1.8238e-02, 5.4382e-01, 4.7864e-01],\n",
            "        [5.6865e-01, 4.4963e-01, 2.8191e-01, 9.8797e-01, 4.8457e-02, 5.0143e-01,\n",
            "         6.7909e-01, 9.8415e-01, 9.3002e-01, 4.3804e-01]])\n",
            "fancy indexing result tensor([ 8,  9, 10])\n",
            "Elements where x1 < 70 or x1 > 90: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69])\n",
            "Even numbers in x1: tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\n",
            "        36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68])\n",
            "Using torch.where: tensor([  0,   1,   2,   3,   4,   5,   6,   7,  16,  18,  20,  22,  24,  26,\n",
            "         28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,  54,\n",
            "         56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,  78,  80,  82,\n",
            "         84,  86,  88,  90,  92,  94,  96,  98, 100, 102, 104, 106, 108, 110,\n",
            "        112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138])\n",
            "tensor([ 1, 20,  3, 40,  5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample tensor\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "# Accessing rows and columns\n",
        "print(x[0, :])  # prints: tensor([1, 2, 3])\n",
        "print(x[:, 1])  # prints: tensor([2, 5, 8])\n",
        "\n",
        "# Slicing\n",
        "print(x[0, 1:3])  # prints: tensor([2, 3])\n",
        "\n",
        "# Modifying elements\n",
        "x[0, 0] = 10\n",
        "print(x)  # prints: tensor([[10,  2,  3], [ 4,  5,  6], [ 7,  8,  9]])\n",
        "\n",
        "# Fancy indexing\n",
        "print(x[[0, 2], [0, 2]])  # prints: tensor([10,  9])\n",
        "\n",
        "# Conditional indexing\n",
        "print(x[(x < 2) | (x > 8)])  # prints: tensor([10,  9])\n",
        "\n",
        "# Finding even numbers\n",
        "print(x[x.remainder(2) == 0])  # prints: tensor([2, 4, 6, 8])\n",
        "\n",
        "# Conditional selection with torch.where()\n",
        "print(torch.where(x > 5, x, 0))  # prints: tensor([[0, 0, 0], [0, 0, 6], [7, 8, 9]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktHZG3RQxU9I",
        "outputId": "9b14378b-1855-4bb7-f006-d3370044e729"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([2, 5, 8])\n",
            "tensor([2, 3])\n",
            "tensor([[10,  2,  3],\n",
            "        [ 4,  5,  6],\n",
            "        [ 7,  8,  9]])\n",
            "tensor([10,  9])\n",
            "tensor([10,  9])\n",
            "tensor([10,  2,  4,  6,  8])\n",
            "tensor([[10,  0,  0],\n",
            "        [ 0,  0,  6],\n",
            "        [ 7,  8,  9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tensor Reshaping**\n",
        "\n",
        "> Learn how to reshape tensors, concatenate them, and change the order of dimensions.\n",
        "\n",
        "- **Reshape with `view()` & `reshape()`:** Change tensor shape without altering data.  \n",
        "- **Transpose & Flatten:** `.t()` transposes, `.contiguous().view(-1)` flattens.  \n",
        "- **Concatenation:** `torch.cat([x1, x2], dim=0/1)` merges tensors along rows/columns.  \n",
        "- **Flattening:** `.view(-1)` converts a tensor into a 1D array.  \n",
        "- **Batch Reshaping:** `.view(batch, -1)` keeps batch size while reshaping.  \n",
        "- **Permute Dimensions:** `.permute(0, 2, 1)` reorders dimensions efficiently.  \n",
        "- **Unsqueeze for New Dimensions:** `.unsqueeze(dim)` adds singleton dimensions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T4IiIZ1Ty-sW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tensor Reshaping\n",
        "===========================================================================\n",
        "\n",
        "> Learn how to reshape tensors, concatenate them, and change the order of dimensions.\n",
        "\n",
        "\n",
        "\n",
        "## Reshaping Tensors\n",
        "\n",
        "#### Using `view()` and `reshape()`\n",
        "\n",
        ">Change the shape of a tensor without altering its data.\n",
        "\n",
        "*   `view()`: Returns a new tensor with the same data but with a different shape.\n",
        "*   `reshape()`: Similar to `view()`, but returns a new tensor with a different shape.\n",
        "\n",
        "#### Transpose and Flatten\n",
        "\n",
        "*   `.t()`: Transposes a tensor.\n",
        "*   `.contiguous().view(-1)`: Flattens a tensor into a 1D array.\n",
        "\n",
        "### Concatenating Tensors\n",
        "\n",
        "*   `torch.cat([x1, x2], dim=0/1)`: Concatenates two tensors along rows (dim=0) or columns (dim=1).\n",
        "\n",
        "### Flattening Tensors\n",
        "\n",
        "*   `view(-1)`: Converts a tensor into a 1D array.\n",
        "\n",
        "### Batch Reshaping\n",
        "\n",
        "*   `view(batch, -1)`: Reshapes a tensor while keeping the batch size intact.\n",
        "\n",
        "### Permuting Dimensions\n",
        "\n",
        "*   `permute(0, 2, 1)`: Reorders the dimensions of a tensor efficiently.\n",
        "\n",
        "### Adding New Dimensions\n",
        "\n",
        "*   `unsqueeze(dim)`: Adds a singleton dimension to a tensor.\n",
        "\n",
        "### Example Use Cases\n",
        "\n",
        "*   Reshaping a tensor to fit a specific model architecture.\n",
        "*   Concatenating multiple tensors to create a larger dataset.\n",
        "*   Flattening a tensor to prepare it for a fully connected layer.\n",
        "*   Permuting dimensions to match the expected input format of a model.\n",
        "*   Adding new dimensions to a tensor to enable broadcasting operations.\n"
      ],
      "metadata": {
        "id": "vcTzrOWSzfnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshape a tensor using view and reshape\n",
        "x = torch.arange (25)\n",
        "x_5x5 = x.view(5,5)\n",
        "print(\"Reshaped to 5x5 using view:\\n\",x_5x5)\n",
        "x_5x5 = x.reshape(5,5)\n",
        "print(\"Reshaped to 5x5 using reshape:\\n\",x_5x5)\n",
        "\n",
        "#Transpose  and flatten  the tensor\n",
        "y = x_5x5.t()\n",
        "print(\"Transposed tensor:\\n\", y)\n",
        "print(\"Flattened tensor:\\n\", y.contiguous().view(-1))\n",
        "\n",
        "#Concatenation example\n",
        "x = torch.rand(5,6)\n",
        "y = torch.rand(5,6)\n",
        "z = torch.cat([x,y], dim=0)\n",
        "print(\"Concatenated tensor:\\n\", z)\n",
        "print(\"Concatenated along dimension 0 (rows):\", torch.cat([x, y], dim=0).shape)\n",
        "print(\"Concatenated along dimension 1 (columns):\", torch.cat([x, y], dim=1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eueFTXWK036i",
        "outputId": "8307def0-0dda-4d65-c8fa-deb30ec7da8c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reshaped to 5x5 using view:\n",
            " tensor([[ 0,  1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14],\n",
            "        [15, 16, 17, 18, 19],\n",
            "        [20, 21, 22, 23, 24]])\n",
            "Reshaped to 5x5 using reshape:\n",
            " tensor([[ 0,  1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14],\n",
            "        [15, 16, 17, 18, 19],\n",
            "        [20, 21, 22, 23, 24]])\n",
            "Transposed tensor:\n",
            " tensor([[ 0,  5, 10, 15, 20],\n",
            "        [ 1,  6, 11, 16, 21],\n",
            "        [ 2,  7, 12, 17, 22],\n",
            "        [ 3,  8, 13, 18, 23],\n",
            "        [ 4,  9, 14, 19, 24]])\n",
            "Flattened tensor:\n",
            " tensor([ 0,  5, 10, 15, 20,  1,  6, 11, 16, 21,  2,  7, 12, 17, 22,  3,  8, 13,\n",
            "        18, 23,  4,  9, 14, 19, 24])\n",
            "Concatenated tensor:\n",
            " tensor([[0.6049, 0.8907, 0.6187, 0.3240, 0.7987, 0.6279],\n",
            "        [0.9034, 0.1171, 0.5957, 0.3890, 0.8713, 0.3883],\n",
            "        [0.7854, 0.9422, 0.6399, 0.1274, 0.0082, 0.5320],\n",
            "        [0.5727, 0.5619, 0.3364, 0.0671, 0.4179, 0.9777],\n",
            "        [0.1002, 0.9354, 0.3704, 0.6749, 0.9268, 0.0834],\n",
            "        [0.9178, 0.8491, 0.1485, 0.0048, 0.8695, 0.1616],\n",
            "        [0.7314, 0.7584, 0.3160, 0.7699, 0.5429, 0.2920],\n",
            "        [0.9889, 0.5871, 0.6257, 0.8531, 0.3713, 0.4225],\n",
            "        [0.1640, 0.7973, 0.6842, 0.4177, 0.1007, 0.0792],\n",
            "        [0.5034, 0.3345, 0.0147, 0.0656, 0.7035, 0.4729]])\n",
            "Concatenated along dimension 0 (rows): torch.Size([10, 6])\n",
            "Concatenated along dimension 1 (columns): torch.Size([5, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshape  with batch dimension\n",
        "batch = 74\n",
        "x = torch.rand(batch,2,5)\n",
        "print(\"Reshaped to (batch,-1):\",x.view(batch,-1).shape)\n",
        "\n",
        "#Permute dimensions\n",
        "z = x.permute(0,2,1)\n",
        "print(\"Permuted tensor shape:\",z.shape)\n",
        "\n",
        "#Unsqueze examples (adding new dimensions)\n",
        "x = torch.arange (20)\n",
        "print (\"original x:\", x)\n",
        "print(\"unsqueeze at dim 0:\", x.unsqueeze(0))\n",
        "print(\"unsqueeze at dim 1:\", x.unsqueeze(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puRyP0is4m_e",
        "outputId": "55401c51-288f-449a-cea1-f64b88f3a64e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reshaped to (batch,-1): torch.Size([74, 10])\n",
            "Permuted tensor shape: torch.Size([74, 5, 2])\n",
            "original x: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19])\n",
            "unsqueeze at dim 0: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "         18, 19]])\n",
            "unsqueeze at dim 1: tensor([[ 0],\n",
            "        [ 1],\n",
            "        [ 2],\n",
            "        [ 3],\n",
            "        [ 4],\n",
            "        [ 5],\n",
            "        [ 6],\n",
            "        [ 7],\n",
            "        [ 8],\n",
            "        [ 9],\n",
            "        [10],\n",
            "        [11],\n",
            "        [12],\n",
            "        [13],\n",
            "        [14],\n",
            "        [15],\n",
            "        [16],\n",
            "        [17],\n",
            "        [18],\n",
            "        [19]])\n"
          ]
        }
      ]
    }
  ]
}